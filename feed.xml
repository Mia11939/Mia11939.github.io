<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mia11939.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mia11939.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-20T04:46:44+00:00</updated><id>https://mia11939.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Conversational Speech Synthesis</title><link href="https://mia11939.github.io/blog/2023/CSS/" rel="alternate" type="text/html" title="Conversational Speech Synthesis"/><published>2023-06-01T10:25:00+00:00</published><updated>2023-06-01T10:25:00+00:00</updated><id>https://mia11939.github.io/blog/2023/CSS</id><content type="html" xml:base="https://mia11939.github.io/blog/2023/CSS/"><![CDATA[<h2 id="project-background">Project Background</h2> <p>With the significant advancements in computer technologies such as signal processing, natural language processing, and speech synthesis, the progression of speech synthesis technology has been greatly propelled. Modern speech synthesis can now generate speech with a naturalness that is close to human level. Although many models have been developed for synthesizing reading styles or highly expressive speech, achieving good quality, these models are largely based on standard speech corpora recorded in controlled environments, such as reading or performing. As a result, they lack the ability to express colloquial speech in real-world scenarios. Therefore, the generation of naturally colloquial speech and the enhancement of the naturalness and authenticity of synthesized speech have become hot topics for researchers.</p> <hr/> <h2 id="conversational-speech-synthesis">Conversational Speech Synthesis</h2> <p>Conversational speech synthesis aims to produce speech that is appropriate for oral communication situations. The existing Text-to-Speech (TTS) technology still falls short in delivering satisfactory performance and immersive experiences in dialogue-oriented tasks due to the following challenges:</p> <ul> <li>an effective method of developing a conversational speech corpus</li> <li>a high performance TTS model of capturing rich prosody in conversations</li> </ul> <p><strong>My work primarily focuses on addressing the second challenge.</strong></p> <p>I am endeavoring to investigate effective methods to bolster the capabilities of <strong>context modeling and comprehension</strong>. This is to ensure that the speech synthesis model can accurately generate context-appropriate emotions and prosody.</p> <hr/> <h2 id="context-modeling-and-comprehension">Context Modeling and Comprehension</h2> <p>Due to the diverse prosody naturally conveying paralinguistic information, including subtle emotions. Hence, how to utilize dialogue context information to enhance the naturalness of the prosody in synthesized speech under spontaneous expression is also a major research focus for scholars.</p> <p>Privious studies have shown that context information modeling in conversational text-to-speech (CTTS) makes a great improvement towards prosody and naturalness of synthesized speech.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/CSS/4.png" class="img-fluid rounded z-depth-1 img-enlarged" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/CSS/5.png" class="img-fluid rounded z-depth-1 img-enlarged" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Previous works on conversational speech synthesis. </div> <blockquote class="block-warning"> <p>In the aspect of Multi-scale</p> </blockquote> <p>However, a majority of developed conversational TTS systems all focus on extracting global information from history dialogue and omitting local prosody information. Prosody contains various context information and is efficient to enhance expressive of dialogue. Hence, besides utterance-level context information extracting, we additional adopt phoneme-level information extracting modules</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/CSS/3.png" class="img-fluid rounded z-depth-1 img-enlarged" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p></p> <p></p> <blockquote class="block-warning"> <p>In the aspect of Multi-Modal</p> </blockquote> <p>Only considering the textual context cannot directly affect the speech synthesis task and we found that the modality gap between them lead to marginal effect. Meanwhile, people speak in different tones even in the same contents or situations. Thus, modeling both modalities is significant.</p> <hr/> <h2 id="the-whole-synthesis-model">The Whole Synthesis Model</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/CSS/1.png" class="img-fluid rounded z-depth-1 img-enlarged" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <hr/> <h2 id="example-of-synthesized-speech">Example of Synthesized Speech</h2> <p>Here demostratie the synthesized speech by proposed method.</p> <ul> <li>Left: baseline FastSpeech2 model.</li> <li>Right: Proposed</li> </ul> <hr/> <p>Sample 1:</p> <div class="caption"> <figure> <audio src="/assets/audio/css/history_3_0_d111.wav" controls=""/> </figure> history dialogue. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/css/3_0_d111.output-v4-base.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/css/3_0_d111.output-v4-base-Wu-Wp.wav" controls=""/> </figure> </div> </div> <div class="caption"> speech synthesized by baseline model and proposed model. </div> <p>Sample 2:</p> <div class="caption"> <figure> <audio src="/assets/audio/css/history_11_0_d1414.wav" controls=""/> </figure> history dialogue. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/css/11_0_d1414.output-v4-base.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/css/11_0_d1414.output-v5-all-predictor-saln.wav" controls=""/> </figure> </div> </div> <div class="caption"> speech synthesized by baseline model and proposed model. </div>]]></content><author><name></name></author><category term="Speech_Synthesis"/><summary type="html"><![CDATA[As a complex techniques in the field of Human-Computer Interaction (HCI), conversational TTS aims to predict suitable style of current utterance from history dialogue and synthesize natural speech.]]></summary></entry><entry><title type="html">Rhythm-controllable Speech Synthesis</title><link href="https://mia11939.github.io/blog/2022/ESS/" rel="alternate" type="text/html" title="Rhythm-controllable Speech Synthesis"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://mia11939.github.io/blog/2022/ESS</id><content type="html" xml:base="https://mia11939.github.io/blog/2022/ESS/"><![CDATA[<h2 id="brief-introduction-of-my-work">Brief Introduction of My Work</h2> <p>The attentional mechanism in end-to-end speech synthesis affects the alignment between text and acoustic feature frames. Meanwhile it is critical for robustness issues, such as repetition, skipping, and attention collapse, as well as improvement of rhythm. With the development of speech synthesis applications, people have higher requirements on the robustness and control ability of speech synthesis systems.</p> <p>Therefore, from aspects of the robustness and controllability, I puts forward a novel attention mechanism, called RC-Attention, to improve the robustness of speech synthesis system, at the same time achieve the rhythm of external control. Specifically, in terms of robustness, the proposed attention algorithm guarantees monotonically increasing alignment and only focuses on one encoder hidden vector at most in each decoding time step. In terms of prosodic control, the proposed attention mechanism introduces additional external rhythm information to participate in alignment calculation, to achieve Fine-grained prosodic control, thus promoting the prosodic naturalness and stylistic expression of synthetic speech. The overall model was extended based on the end-to-end speech synthesis framework Tacotron2, and experiment was conducted using a self-built corpus. </p> <details><summary>Click here to know more</summary> <p>Current end-to-end speech synthesis systems have the ability to generate high-quality and human-like speech. Hence, speech synthesis application scenarios are becoming more and more diversified. All of these scenarios have two basic demands on synthesis systems. Firstly, speech corresponding to text can be synthesized accurately and does not exist repetition, skipping, or gibberish. This demand focuses on the <strong>robustness of synthesis system</strong>. Secondly, in addition to accuracy, people would expect synthesized speech is capable of <strong>generating natural rhythm of human</strong>. This demand focuses on model’s ability of modeling and controlling of rhythm. Both of demands can be satisfied by a well-designed attention mechanism.</p> <p>The problems of robustness in speech synthesis arise from extremely unequal length between text sequence and acoustic feature sequence. In the process of upsampling, if mapping relationship from text sequences to acoustic feature sequences, named alignment matrix, is not learned well, two types of alignment problems are likely to occur. One is attention collapse which will lead to gibberish. The other is blurred alignment which means acoustic model fails to focus on a single input token in a decoding step, leading to skipping and repeating.</p> <p>Some studies propose robust attention mechanisms in order to alleviate difficulty of alignment learning and accumulation of errors in the inference stage. Raffel et al. propose Monotonic Attention. The core idea is that when decoding the current timestep, model only needs to decide whether to focus on the current phoneme or move the focus forward. However, completeness of alignment cannot be satisfied, which may lead to skipping. He et al. propose Stepwise Monotonic Attention based on Monotonic Attention. By adding a constraint of forward stride to monotonic attention, the method ensures that each phoneme can be covered by at least one frame of mel spectrogram. Another novel attention is propoesd by Graves et al. , named GMM Attention which uses multiple mixed Gaussian distributions to model alignment. Meanwhile, in order to make sure monotonicity, the mean value of mixed Gaussian is constrained to increase as decoding time goes by. Many speech synthesis systems use these attentions to speed up converge of training and improve robustness of synthesis .</p> <p>Methods of rhythm control are investigated by a few studies. For example, Zhang et al. propose a forward algorithm, named Forward Attention which contains a transition agent to achieve rhythm control. However, robustness may be damaged after adding the external rhythm control agent. In the inferring phase, adjusting value of transition agent by human may leads to accumulation of errors.</p> <p>Therefore, in this work, we combine advantages of these attentions and propose a novel attention mechanism, called Rhythm-controllable Attention (RC-Attention), to satisfy demands of robustness and rhythm control. Proposed attention mechanism can improve robustness compared with other advanced attention mechanisms, especially in long sentences synthesis, meanwhile utilizing external duration information to synthesize speech with more natural rhythm.</p> </details> <hr/> <h2 id="lets-begin-from-attention-mechanisms">Let’s begin from Attention Mechanisms</h2> <p><strong>Why Study Attention Mechanisms?</strong></p> <p>The impact of attention alignment algorithms can be divided into three aspects: robustness of the synthesis system, expression of rhythmic patterns, and model convergence speed, among others.</p> <p>Firstly, in terms of robustness, applications involving human-machine interaction such as intelligent robots and smart homes have high requirements for the robustness of speech synthesis systems. They expect the synthesized audio to remain coherent and fluent even in extreme boundary cases. However, even with the use of large-scale training data for synthesizing systems, some extreme boundary cases can still arise during actual application. For example, during speech synthesis training, the average length of the training data may be 14 characters, but the synthesis stage requires synthesizing text of 200 characters. Speech synthesis systems with poor robustness tend to produce phenomena such as repetition, skipped words, and incoherent speech, known as synthesis robustness issues. Designing effective attention mechanisms to enhance the alignment of text-to-speech features can effectively improve the robustness of the synthesis system.</p> <p>Secondly, in terms of expressing rhythmic patterns, attention alignment influences the rhythm and prosody of synthesized speech. By using attention, control over the rhythm and prosody of the synthesized speech can be achieved, thereby enhancing its naturalness and conveying higher-level non-semantic information such as emotion, style, or intent. Due to diverse applications and market development, the demands for synthesized speech have been increasing. In addition to ensuring fluency and audio quality, there is a need for synthesized speech with rich rhythms and multiple styles. Speech synthesis systems with diverse rhythms and styles can be applied in various scenarios such as virtual assistants, call centers, voiceovers for movies and games, smart homes, narration of audiobooks, and online education. In audiobooks, synthesizing emotionally appropriate speech that aligns with the contextual context of the text can enhance the rendering power of the content. In smart homes or intelligent voice assistants like Xiaodu or Xiaobing, applying emotional synthesis technology enables machines to exhibit human-like emotions, providing various intelligent and convenient functions to users. In online education applications, such as computer-assisted teaching of Chinese as a foreign language, intonation and rhythm in pronunciation can help learners grasp key points better, enhancing memory and comprehension of knowledge.</p> <figure> <picture> <img src="/assets/img/ESS/4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Example of repetition, skipping, and attention collapse </div> <p>Lastly, in terms of training convergence speed, different attention mechanisms exhibit significant differences in the speed of alignment convergence. Accelerating the convergence speed helps save training time costs and energy consumption. For example, compared to the Location-sensitive Attention, the GMMv2b attention mechanism can achieve faster convergence speed, resulting in better accuracy and fluency of synthesized speech within the same number of training steps.</p> <p>In summary, research on attention-based alignment algorithms holds significant value and importance. They play an important role in improving the robustness of synthesis systems, accelerating convergence speed, and enhancing the expression of rhythmic patterns in synthesized speech. Moreover, it has become a major research focus in the field of speech synthesis in recent years.。</p> <h2 id="example-of-synthesized-samples">Example of Synthesized samples</h2> <blockquote> <p>Proposed attention mechanism can improve robustness compared with other advanced attention mechanisms, especially in long sentences synthesis</p> </blockquote> <figure> <picture> <img src="/assets/img/ESS/1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The structure of my attention </div> <p>The complex sentences used in this experiment were selected from recent press releases. Each sentence contains an average of 147 Chinese characters, which is considered lengthy compared to the average length of 14 characters in the training text.</p> <ul> <li>Left: Baseline</li> <li>Right: Proposed</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/base/angery_0.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/proposed/angery_0.wav" controls=""/> </figure> </div> </div> <div class="caption"> transcription: 一开始她就从长远考虑，从消费者角度出发，注重原料的质量。 她从不贪便宜购买使用“地沟油”等劣质产品； 从不使用过期的食品材料。 她专门购买了冰箱、消毒柜，对碗筷餐具不但冲洗干净而且进行充分消毒。 同时，饭菜明码标价，而且定价合理，童叟无欺，不论是熟人还是生人、本县人还是外地人，她都一视同仁，诚恳相待，服务周到。 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/base/happy_2.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/proposed/happy_2.wav" controls=""/> </figure> </div> </div> <div class="caption"> transcription: 刘晓萍连续多年，利用休息时间给毕业生做毕业纪念册。 这些纪念册记录着学生在学校生活的点点滴滴，它们成了学生最珍贵纪念品。 特别是二零一五届的学生纪念册的成功制作，荣获社会各界的赞誉。 仅仅选择照片这一项任务耗费了很多时间，周末加班选，晚上熬夜写。 为了做好这本纪念册，她常常熬到深夜甚至通宵。她说既然要做就做得最好。 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/base/neutral_26.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/proposed/neutral_26.wav" controls=""/> </figure> </div> </div> <div class="caption"> transcription: 在当时农村，大多数农民对农保政策了解还不透彻，邢墨峰就带着工作组开始围着一百多个村庄挨个转，走访了三千多个重点户。 为了让农民充分认识新农保政策的优越性，邢墨峰从网络、杂志、报纸上查阅了大量资料，亲自起草编写了系列宣传材料。 考虑到一部分农民不识字，他又把新农保宣传材料编成“音乐快板”，下村播放。 </div> <hr/> <blockquote> <p>Proposed attention mechanism utilizs external duration information to synthesize speech with more natural rhythm.</p> </blockquote> <p></p> <figure> <picture> <img src="/assets/img/ESS/5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/speed/happy_16.wav" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/speed/happy_13.wav" controls=""/> </figure> </div> </div> <div class="caption"> Fine-grained display of Mel spectrograms with precise control over rhythm and tempo. In the upper image, the rhythmic tempo of the word "真是" is slow, while in the lower image, the rhythmic tempo of the word "真是" is fast. The audio on the left corresponds to the upper image, while the audio on the right corresponds to the lower image. </div> <h2 id="self-built-corpus">Self-Built Corpus</h2> <p>The related validation experiment is conducted based on a self-built emotional corpus, which includes four types of emotional tags: happiness, anger, neutrality, and sadness. If you are interested in my corpus, please feel free to reach out to me via email.</p> <table> <thead> <tr> <th>Emotion</th> <th style="text-align: center">Size</th> <th style="text-align: right">Example</th> </tr> </thead> <tbody> <tr> <td>neutral</td> <td style="text-align: center">2334</td> <td style="text-align: right">眼中光芒强盛，看向对面的两个孩子时</td> </tr> <tr> <td>happy</td> <td style="text-align: center">2334</td> <td style="text-align: right">眼中射出两道光束</td> </tr> <tr> <td>angry</td> <td style="text-align: center">2332</td> <td style="text-align: right">镇中的人闻言，全都变色</td> </tr> <tr> <td>sad</td> <td style="text-align: center">2332</td> <td style="text-align: right">从开始到现在，数次大战这个山村的孩子</td> </tr> </tbody> </table> <p></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/gt/20211207_164732_760.flac" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/gt/20211207_165023_004.flac" controls=""/> </figure> </div> </div> <div class="caption"> Right: Happy. Left: Angry </div> <p></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/gt/20211207_165751_851.flac" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/RCAttention/gt/20211218_194543_095.flac" controls=""/> </figure> </div> </div> <div class="caption"> Right: Neutral. Left: Sad </div> <h2 id="vedio">Vedio</h2> <p>If you wish to delve into more details, I recommend you to watch the detailed introduction video below, or click on the <a href="https://arxiv.org/abs/2306.02593">link</a> to read the original academic paper.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/RCAttention.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A detailed introduction video </div> ]]></content><author><name></name></author><summary type="html"><![CDATA[Rhythm-controllable Attention with High Robustness for Long Sentence Speech Synthesis]]></summary></entry><entry><title type="html">Three-dimensional Human-computer Interaction Model Design for Chinese Pronunciation</title><link href="https://mia11939.github.io/blog/2021/3D/" rel="alternate" type="text/html" title="Three-dimensional Human-computer Interaction Model Design for Chinese Pronunciation"/><published>2021-04-01T21:01:00+00:00</published><updated>2021-04-01T21:01:00+00:00</updated><id>https://mia11939.github.io/blog/2021/3D</id><content type="html" xml:base="https://mia11939.github.io/blog/2021/3D/"><![CDATA[<h2 id="project-background">Project Background</h2> <p>In the context of the global outbreak of the COVID-19 virus, the development of artificial intelligence in the field of education is gaining momentum, and online teaching is gradually becoming a learning method that everyone is willing to accept. The new teaching method also poses new challenges to the traditional classroom teaching model. Traditional classroom pronunciation teaching mainly relies on teachers to demonstrate specific actions through language, teaching aids, or body, allowing students to comprehend on their own. This teaching method is highly ambiguous and difficult to meet the needs of online teaching and computer-assisted pronunciation teaching feedback.</p> <p>In terms of pronunciation teaching, in the absence of teaching aids and face-to-face demonstration and guidance from teachers, how to allow learners to quickly understand the essentials of phonetics learning and carry out effective imitation learning has become the direction of thinking for phonetics teachers under the new situation.</p> <p>In recent years, computer-assisted pronunciation teaching technology (CAPT) has begun to be applied in pronunciation teaching online platforms, and various attempts have been made to better give learners feedback on pronunciation problems. Feedback on pronunciation scores through GOP (Goodness of pronunciation) can help learners to a certain extent to recognize the pros and cons of their pronunciation. For example, feedback on pronunciation problems in the form of pronunciation attributes can make learners more clearly recognize where their pronunciation problems are and how to correct them. However, without the on-site guidance of the teacher, through textual descriptions or overly professional guidance methods, learners find it difficult to capture changes in key pronunciation parts, and even cannot understand the specific designation of various parts of the oral cavity, making it difficult to produce correct pronunciation.</p> <p>In terms of pronunciation phonetics research, some scholars have tracked the movement trajectories of the lips and tongue using EMA to obtain the lip and tongue dynamic data of each phoneme. From the perspective of pronunciation feedback, this method can reflect the oral changes of the target phoneme in a relatively intuitive way. However, the cost and difficulty of data collection and analysis in this way are high. The main manifestations are that the need to stick sensors to the specific positions of the speaker’s pronunciation organs will bring a strong discomfort to the speaker and affect the naturalness of pronunciation; the sensor electrodes fall off, which may cause data collection failure or inaccurate collection; the number of sensor electrodes is limited, and data modeling may face data sparsity and other problems. And the pronunciation data obtained in this way is difficult to analyze, and it is difficult to meet the flexible needs of pronunciation teaching.</p> <blockquote class="block-warning"> <p>This project is based on a three-dimensional anatomical model of the human head, creating animations and designing interactions in Unity. Considering the situation where the phonetic organs inside the mouth are not fully visible or completely invisible, we start from the perspective of visualizing the phonetic organs. We construct a visualized three-dimensional virtual human head and its oral system that can generate synchronous speech animations, in order to achieve the best effect in learning phonetics.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/3D/2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/3D/1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="practical-significance">Practical Significance</h2> <p>In response to the situation where the articulatory organs in the oral cavity are not fully visible or completely invisible, from the perspective of articulatory visualization, a three-dimensional virtual human head and its oral system capable of producing synchronized speech animation are constructed, so that the pronunciation method of Chinese phonetic symbols can be intuitively represented by a three-dimensional animation model. This project is based on a three-dimensional human head model, using knowledge of anatomy and pronunciation visualization to create a human oral anatomical simulation speech animation.</p> <ul> <li>Assisting in the classroom teaching of Chinese for international students studying in China. The visualization of this 3D virtual human head and its oral system through voice animation can aid teachers in classroom teaching, post-class evaluation feedback, and students’ online learning. By observing the way of vocalization in the simulated image animation, it can help Chinese learners master the correct way of vocalization.</li> <li>Helping in the teaching of Chinese pronunciation for children, the vivid 3D animation helps children understand and master Chinese pronunciation. At the same time, the 3D visualization of pronunciation, language teaching, and even human-computer interaction will play a positive role in promoting, with important theoretical significance and potential application prospects.</li> <li>For people with hearing impairments, they often cannot make a sound because they cannot hear the sound and do not know how to vocalize. The 3D visualized pronunciation animation solves this problem well, allowing people with hearing impairments to see how the vocal organs make sounds.</li> <li>The language problems of children with intellectual disabilities are becoming increasingly prominent. This system can be used to guide children with hearing impairments in tongue pronunciation position training. To overcome the invisibility problem in the tongue pronunciation movement process in the current language learning of children with hearing impairments, and improve the effect of pronunciation training for children with hearing impairments.</li> <li>At present, there is a lack of research on the visualization of Chinese pronunciation, especially through the visualization of three-dimensional models. Therefore, the generation of synchronized voice animation’s visualized 3D virtual human head and its oral system is very important in language learning, especially in second language learning and speech correction. In addition, this system can also be extended to the visualization of pronunciation teaching in English phonetics, Japanese fifty-sound chart, and so on.</li> </ul> <h2 id="project-content">Project Content</h2> <blockquote class="block-tip"> <p>Overall Goal: To construct a user-friendly, three-dimensional virtual human head and oral system that can generate synchronized voice animations.</p> </blockquote> <blockquote class="block-warning"> <p>In the aspect of animation </p> </blockquote> <ul> <li>Based on the muscle movements of human oral pronunciation, construct and design three-dimensional animations of the movements of each vocal organ.</li> <li>For vocal organs such as teeth, hard palate, and lower jaw that only undergo minor deformations or even no deformation during pronunciation, consider them as rigid bodies and simulate their movements. For vocal organs such as the tongue and soft palate that undergo significant deformations during pronunciation, attempt to simulate their deformation effects. By calling the Unity toolkit, simulate the movement of special soft muscles like the tongue.</li> <li>Meanwhile, in terms of data collection, we try to visualize the Chinese pronunciation process by driving a three-dimensional physiological model with data from a Chinese Electromagnetic Articulograph (EMA).</li> </ul> <blockquote class="block-warning"> <p>In the aspect of interaction design. </p> </blockquote> <ul> <li>Establish a basic interactive framework. Achieve human-computer interaction effects through script programming, including the movement and rotation of the head perspective, selective semi-transparency of muscles, and other interactions.</li> <li>Optimize voice animation and interaction process. For the situation where the pronunciation organs inside the mouth are not fully visible or completely invisible, from the perspective of visualization of the pronunciation organs, better interaction design is carried out on the model so that users can more clearly observe the movement of the pronunciation parts. Optimization of interaction includes: reserving interfaces, reducing the number of control points, adding visual comparison sections for Chinese and English pronunciation, and so on.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/3D_2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> A Dynamic 3D Pronunciation Teaching Model Example </div> <h2 id="innovation-description">Innovation Description</h2> <ul> <li>Combining knowledge in the field of anatomy, a pronunciation feedback system has been developed. A professional anatomical human head model is used for the simulation and teaching of Chinese pronunciation. The pronunciation animation is based on a physiological anatomical three-dimensional model, involving the simulation of the movement of the pronunciation organs in the oral cavity. For pronunciation organs such as teeth, hard palate, and lower jaw, which only produce minor shape changes or even no shape changes during pronunciation, they are treated as rigid bodies and simulated by movements such as rotation and scaling. For pronunciation organs such as the tongue and soft palate, which produce a large amount of shape changes during pronunciation, the Free-Form Deformation (FFD) attribute is added to simulate their shape change effects.</li> <li>Innovation in application scenarios. There are almost no Chinese teaching software or programs on the market based on three-dimensional human head anatomical models, most of them are two-dimensional teaching animations. The changes in the most important pronunciation organ in the oral cavity - the tongue, cannot be seen when simulated with two-dimensional animations or real people. Now, we have incorporated knowledge of anatomy to create a user-friendly interactive Chinese pronunciation simulation system to achieve precise simulation effects. So far, no one has done this, and we have currently implemented basic vocal animations and simple interactions using Unity technology.</li> <li>This project applies anatomical three-dimensional animation to the scenario of Chinese language teaching, realizing innovation in application scenarios.</li> </ul>]]></content><author><name></name></author><category term="Human-Computer-Interaction"/><summary type="html"><![CDATA[Three-dimensional Human-computer Interaction Model Design for Chinese Pronunciation]]></summary></entry></feed>