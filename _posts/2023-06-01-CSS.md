---
layout: post
title:  Conversational Speech Synthesis
date: 2023-06-01 10:25:00
description: As a complex techniques in the field of Human-Computer Interaction (HCI), conversational TTS aims to predict suitable style of current utterance from history dialogue and synthesize natural speech.
tags: Speech_Synthesis
featured: true
toc:
  sidebar: left
# categories: sample-posts
---

## Project Background
With the significant advancements in computer technologies such as signal processing, natural language processing, and speech synthesis, the progression of speech synthesis technology has been greatly propelled. Modern speech synthesis can now generate speech with a naturalness that is close to human level. Although many models have been developed for synthesizing reading styles or highly expressive speech, achieving good quality, these models are largely based on standard speech corpora recorded in controlled environments, such as reading or performing. As a result, they lack the ability to express colloquial speech in real-world scenarios. Therefore, the generation of naturally colloquial speech and the enhancement of the naturalness and authenticity of synthesized speech have become hot topics for researchers.

*** 

## Conversational Speech Synthesis
Conversational speech synthesis aims to produce speech that is appropriate for oral communication situations. The existing Text-to-Speech (TTS) technology still falls short in delivering satisfactory performance and immersive experiences in dialogue-oriented tasks due to the following challenges:
<ul>
    <li>an effective method of developing a conversational speech corpus</li>
    <li>a high performance TTS model of capturing rich prosody in conversations</li>
</ul>

**My work primarily focuses on addressing the second challenge.** 

I am endeavoring to investigate effective methods to bolster the capabilities of **context modeling and comprehension**. This is to ensure that the speech synthesis model can accurately generate context-appropriate emotions and prosody.

<!-- <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="assets/audio/epicaly-short-113909.mp3" controls=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls=true %}
    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div> -->

*** 

## Context Modeling and Comprehension

Due to the diverse prosody naturally conveying paralinguistic information, including subtle emotions. Hence, how to utilize dialogue context information to enhance the naturalness of the prosody in synthesized speech under spontaneous expression is also a major research focus for scholars.

Privious studies have shown that context information modeling in conversational text-to-speech (CTTS) makes a great improvement towards prosody and naturalness of synthesized speech. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/CSS/4.png" class="img-fluid rounded z-depth-1 img-enlarged" %}
    </div>
<div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/CSS/5.png" class="img-fluid rounded z-depth-1 img-enlarged" %}
</div>
</div>
<div class="caption">
    Previous works on conversational speech synthesis.
</div>

> In the aspect of Multi-scale
{: .block-warning }

However, a majority of developed conversational TTS systems all focus on extracting global information from history dialogue and omitting local prosody information. Prosody contains various context information and is efficient to enhance expressive of dialogue. Hence, besides utterance-level context information extracting, we additional adopt phoneme-level information extracting modules


<!-- , that is Text Phoneme-level Module (TPM) and Wave Phoneme-level Module (WPM) which adopt multi-head attention mechanism to identify significant parts of historical dialogue. Overall, in this paper we introduce C3M-TTS, an end-to-end conversational multi-scale multi-modal multi-style text-to-speech system, aims to utilize sufficient context information and enhance prosody expression. Specifically, we conduct experiments on a conversational speech corpus, DailyTalk. Experimental results show that the model mixed with fine-grained context information achieve better prosody performance and naturalness than those not. -->

<div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/CSS/3.png" class="img-fluid rounded z-depth-1 img-enlarged" %}
</div>

<p></p>
<p></p>

> In the aspect of Multi-Modal
{: .block-warning }

Only considering the textual context cannot directly affect the speech synthesis task and we found that the modality gap between them lead to marginal effect. Meanwhile, people speak in different tones even in the same contents or situations. Thus, modeling both modalities is significant. 

*** 

## The Whole Synthesis Model

<div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/CSS/1.png" class="img-fluid rounded z-depth-1 img-enlarged" %}
</div>

<!-- > Overall Goal: To construct a user-friendly, three-dimensional virtual human head and oral system that can generate synchronized voice animations.
{: .block-tip } -->

*** 

## Example of Synthesized Speech
Here demostratie the synthesized speech by proposed method.

<ul>
    <li>Left: baseline FastSpeech2 model.</li>
    <li>Right: Proposed</li>

</ul>

*** 

Sample 1:
<div class="caption">
    {% include audio.html path="assets/audio/css/history_3_0_d111.wav" controls=true %}
    history dialogue.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="assets/audio/css/3_0_d111.output-v4-base.wav" controls=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="assets/audio/css/3_0_d111.output-v4-base-Wu-Wp.wav" controls=true %}
    </div>
</div>
<div class="caption">
    speech synthesized by baseline model and proposed model.
</div>

Sample 2:
<div class="caption">
    {% include audio.html path="assets/audio/css/history_11_0_d1414.wav" controls=true %}
    history dialogue.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="assets/audio/css/11_0_d1414.output-v4-base.wav" controls=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include audio.html path="assets/audio/css/11_0_d1414.output-v5-all-predictor-saln.wav" controls=true %}
    </div>
</div>
<div class="caption">
    speech synthesized by baseline model and proposed model.
</div>