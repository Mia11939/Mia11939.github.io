<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Yayue Deng</title> <meta name="author" content="Yayue Deng"> <meta name="description" content=""> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mia11939.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Experience</a> </li> <link href="/assets/css/main_add.css" rel="stylesheet"> <link href="https://fonts.googleapis.com/css?family=Lato|Oswald" rel="stylesheet"> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">cv</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/assets/pdf/cv_zh.pdf">cv_download</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yayue</span> Deng </h1> <p class="desc"><a href="https://ai.bupt.edu.cn/" rel="external nofollow noopener" target="_blank">School of Artificial Intelligence, Beijing University of Posts and Telecommunications</a></p> </header> <article> <div class="profile float-left"> <figure> <picture> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>(+86)18810956816</p> <p>yayue.deng@bupt.edu.cn</p> <p>No.10 Xitucheng Road, Haidian District, Beijing, China 100876</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%61%79%75%65.%64%65%6E%67@%62%75%70%74.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/Mia11939" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://twitter.com/dngydn482045" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> The best way to reach me is through email, as I check it regularly throughout the day. </div> </div> </div> <div class="clearfix"> <p>Hello! My name is Yayue Deng. I am a dedicated Artificial Intelligence Master’s student at Beijing University of Posts and Telecommunications (BUPT), with an expected graduation date in June 2025.</p> <p>My academic focus is on Artificial Intelligence and Machine Learning, with a particular interest in <strong>Speech Synthesis and Natural Language Processing</strong>. My ultimate goal is to leverage my expertise in these areas to develop a conversational chatbot that exhibits both emotional and intellectual intelligence. I am currently researching Speech Synthesis (TTS) and Spoken Dialogue Systems as part of my Master’s degree, which aligns with my future aspirations.</p> <p>In addition to this primary line of research, I am also interested in:</p> <p>  - Refinement of models in the field of financial economics, and the application of domain-specific language models.</p> <p>  - Macroeconomics, Machine Learning, and Finance</p> <p>You can also read more about my research experience and my background by <a href="blog/">clicking</a> or <a href="https://scholar.google.com/citations?user=N_iA1hoAAAAJ&amp;hl=zh-CN" rel="external nofollow noopener" target="_blank">google scholar</a>.</p> <p><strong>My future plan is to pursue a doctoral degree. If you are interested in my field of research, please feel free to contact me at yayue.deng@bupt.edu.cn or Twitter!</strong></p> </div> <p></p> <br> <h2>news</h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 19, 2023</th> <td> My paper “CONCSS: Contrastive-based Context Comprehension for Dialogue-appropriate Prosody in Conversational Speech Synthesis” Accepted by ICASSP 2024.<img class="emoji" title=":smiley_cat:" alt=":smiley_cat:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f63a.png" height="20" width="20"> <img class="emoji" title=":raised_hands:" alt=":raised_hands:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f64c.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jun 19, 2023</th> <td> Passing Test for English Majors-Band 8(TEM-8) </td> </tr> <tr> <th scope="row">Sep 1, 2022</th> <td> Qualified for the recommendation for postgraduate study, joined Professor Li Ya’s research group, and started the postgraduate stage.<img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Nov 8, 2021</th> <td> I stayed a six-month internship at the Artificial Intelligence Department of China Mobile Research Institute. </td> </tr> <tr> <th scope="row">Aug 1, 2021</th> <td> Starting the study of the ‘Prospect Talents in Digital Economy’ program at Guanghua School of Management, Peking University. </td> </tr> </table> </div> </div> <p></p> <br> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/CONCSS.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="CONCSS.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="deng2023concss" class="col-sm-8"> <div class="title">CONCSS: Contrastive-based Context Comprehension for Dialogue-appropriate Prosody in Conversational Speech Synthesis</div> <div class="author"> Yayue Deng, Jinlong Xue, Yukang Jia, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Qifei Li, Yichen Han, Fengping Wang, Yingming Gao, Dengfeng Ke, Ya Li' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2312.10358</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2312.10358" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/CONCSS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/cmcu.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cmcu.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="deng2023cmcu" class="col-sm-8"> <div class="title">CMCU-CSS: Enhancing Naturalness via Commonsense-based Multi-modal Context Understanding in Conversational Speech Synthesis</div> <div class="author"> Yayue Deng, Jinlong Xue, Fengping Wang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yingming Gao, Ya Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 31st ACM International Conference on Multimedia</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/CMCU-Poster.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/icassp2023.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="icassp2023.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10096905" class="col-sm-8"> <div class="title">M2-CTTS: End-to-End Multi-Scale Multi-Modal Conversational Text-to-Speech Synthesis</div> <div class="author"> Jinlong Xue, Yayue Deng, Fengping Wang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Ya Li, Yingming Gao, Jianhua Tao, Jianqing Sun, Jiaen Liang' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.02269" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/ICASSP2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ICASSP49357.2023.10096905"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICASSP49357.2023.10096905" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/RC-Attention.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="RC-Attention.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10037822" class="col-sm-8"> <div class="title">Rhythm-controllable Attention with High Robustness for Long Sentence Speech Synthesis</div> <div class="author"> Dengfeng Ke, Yayue Deng, Yukang Jia, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Jinlong Xue, Qi Luo, Ya Li, Jianqing Sun, Jiaen Liang, Binghuai Lin' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In 2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.02593" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/Rhythm_controllable_Attention.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/ISCSLP57327.2022.10037822"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ISCSLP57327.2022.10037822" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> </div> <p></p> <br> <h2>Experience</h2> <div class="container row"> <div class="item"> <i class="vertical-line"></i> <h2 class="item-date">07/2020 - 04/2021</h2> <div class="card-panel"> <h3 class="card-title"> Three-dimensional Human-computer Interaction Model Design for Chinese Pronunciation </h3> <p> </p> <p>Act as an excellent project leader of National College Students’ Innovation and Entrepreneurship Training Program. The project is used to help the Chinese learners understand the exact location and method of the phoneme articulation intuitively. This project introduced a dynamic three dimensional (3D) head model built based on the knowledge of anatomy and the theory of distinctive features.</p> <figure> <picture> <img src="/assets/img/3D/2.png" width="500px" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="/assets/pdf/3Dmodel20200802.pdf">paper</a></p> </div> </div> <div class="item"> <i class="vertical-line"></i> <h2 class="item-date">04/2021 - 09/2021</h2> <div class="card-panel"> <h3 class="card-title"> Detecting Moral Sentiment In Text </h3> <p> </p> <p>Expressions of moral sentiment play a fundamental role in political framing, social solidarity, and basic human motivation. This research aims at detecting and understanding the moral biases in text.</p> <figure> <picture> <img src="/assets/img/NLP/1.png" width="500px" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://aclanthology.org/2021.ccl-1.49/" rel="external nofollow noopener" target="_blank">paper</a></p> </div> </div> <div class="item"> <i class="vertical-line"></i> <h2 class="item-date">09/2021 - 12/2022</h2> <div class="card-panel"> <h3 class="card-title"> Expressive Speech Synthesis Research </h3> <p> </p> <p>My research on expressiveness text-to-speech aims to synthesize intelligible and natural speech,which focuses on prosody, emotion, and style modeling.</p> <figure> <picture> <img src="/assets/img/ESS/1.png" width="500px" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://arxiv.org/abs/2306.02593" rel="external nofollow noopener" target="_blank">paper</a></p> </div> </div> <div class="item"> <i class="vertical-line"></i> <h2 class="item-date">11/2021 - 06/2022</h2> <div class="card-panel"> <h3 class="card-title"> Fake Speech Generation </h3> <p> </p> <p>Participated in the First Audio Deep Synthesis Detection Challenge (ADD 2022) in ICASSP 2022, responsible for utilizing non-autoregressive speech synthesis system to generate fake speech As Winter intern in China Mobile Research Institute,Beijing, Artificial Intelligence Department</p> <p><a href="https://signalprocessingsociety.org/publications-resources/data-challenges/audio-deepfake-detection-icassp-2022" rel="external nofollow noopener" target="_blank">official website</a></p> </div> </div> <div class="item"> <i class="vertical-line"></i> <h2 class="item-date">09/2022 - 12/2023</h2> <div class="card-panel"> <h3 class="card-title"> Conversational Speech Synthesis </h3> <p> </p> <p>Participated in the “interactive and empathetic spoken speech generation for mental health counseling” project , which is supported by Open Project Program of the National Laboratory of Pattern Recognition (NLPR).</p> <figure> <picture> <img src="/assets/img/CSS/1.png" width="500px" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://arxiv.org/pdf/2305.02269.pdf" rel="external nofollow noopener" target="_blank">paper</a></p> <p>Granted four invention patents abd approved for a project by the National Natural Science Foundation of China</p> <figure> <picture> <img src="/assets/img/publication_preview/CONCSS.jpg" width="500px" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://arxiv.org/pdf/2312.10358.pdf" rel="external nofollow noopener" target="_blank">paper</a></p> </div> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Yayue Deng. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>